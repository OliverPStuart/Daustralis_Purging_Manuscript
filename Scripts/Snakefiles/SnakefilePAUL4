### Snakefile for PAUL4 analysis. The pipeline takes reads, maps and
### deduplicates them, calls variants in parallel with FreeBayes, and filters
### them. Then it converts the filtered vcfs into psmc and plink formats. It
### takes the plink format and identifies ROH across a few different parameter
### settings. It takes the psmc format and performs psmc for the whole genome
### and then performs 100 bootstraps. Finally, it calculates H for each
### scaffold.

### May add a rule to estimate recombination map using ismc.

HOME_DIR = "/mnt/data/dayhoff/home/scratch/groups/mikheyev/LHISI"
READ_DIR = HOME_DIR + "/Reads/WGS"
ALN_DIR = HOME_DIR + "/Alignments/WGS"
REF_DIR = HOME_DIR + "/References"
OUT_DIR = HOME_DIR + "/Analysis/PAUL4"
SOFT_DIR = "/mnt/data/dayhoff/home/u6905905/"

### Modify this line to exclude or include scaffolds as necessary
### Only include autosomes for this
CHROMS = ["CM056993.1","CM056994.1","CM056995.1","CM056996.1","CM056997.1","CM056998.1","CM056999.1","CM057000.1","CM057001.1","CM057002.1","CM057003.1","CM057004.1","CM057005.1","CM057006.1","CM057007.1","CM057008.1"]
#CHROMS = ["CM057008.1"] # tester line
### Define chunks for calling later
NCHUNKS = 30
CHUNKS = list(range(1,NCHUNKS+1))

### Final rule
rule all:
	input:
		OUT_DIR + "/GENOME_WIDE_H.txt",
		expand(OUT_DIR + "/PAUL4_{chrom}.bed",chrom=CHROMS)

### Map the files to the reference and follow samtools pipeline to remove duplicates
rule map:
	input:
		ref = REF_DIR + "/LHISI_Scaffold_Assembly_Polished.fasta",
		r1 = READ_DIR + "/PAUL4_R1.fastq.gz",
		r2 = READ_DIR + "/PAUL4_R2.fastq.gz"
	output:
		ALN_DIR + "/PAUL4_WGS.bam"
	params:
		dir = ALN_DIR
	threads: 46
	shell:
		"""
		bwa mem -t {threads} -R "@RG\\tID:PAUL4\\tSM:PAUL4\\tLB:NEXTFLEX\\tPL:ILLUMINA" \
		{input.ref} {input.r1} {input.r2} |
		samtools fixmate -m -r -u - - | \
		samtools sort -u -@ {threads} -T {params.dir}/PAUL4_WGS_1 - | \
		samtools markdup -@ {threads} -T {params.dir}/PAUL4_WGS_2 --reference {input.ref} -r -f {params.dir}/PAUL4_WGS_dupstats.txt - - | \
		samtools view -bh > {output}
		"""

### Index the bam files
rule index:
	input:
		ALN_DIR + "/PAUL4_WGS.bam"
	output:
		ALN_DIR + "/PAUL4_WGS.bam.bai"
	threads: 46
	shell:
		"""
		samtools index -@ {threads} {input}
		"""

### Rule to get depth statistics
rule get_depth:
	input:
		bam = ALN_DIR + "/PAUL4_WGS.bam",
		index = ALN_DIR + "/PAUL4_WGS.bam.bai"
	output:
		OUT_DIR + "/PAUL4_WGS.regions.bed.gz"
	threads: 46
	shell:
		"""
		mosdepth -t {threads} -n -Q 10 -b 10000 PAUL4_WGS {input.bam}
		rm *mosdepth*
		"""

### Rule to make bed files to split up calling
rule generate_freebayes_regions:
	input:
		ref_idx = REF_DIR + "/LHISI_Scaffold_Assembly_Polished.fasta",
		index = REF_DIR + "/LHISI_Scaffold_Assembly_Polished.fasta.fai",
	output:
		regions = expand(REF_DIR + "/bed_files/genome.{chrom}.region.{i}.bed", chrom=CHROMS, i=CHUNKS)
	params:
		chroms = CHROMS,
		chunks = NCHUNKS,
		soft = SOFT_DIR,
		ref_dir = REF_DIR
	shell:
		"""
		{params.soft}/freebayes/scripts/fasta_generate_regions.py \
		--chunks \
		--chromosomes {params.chroms} \
		--bed {params.ref_dir}/bed_files/genome \
		{input.index} \
		{params.chunks}
		"""

### Rule to do calls
rule variant_calling_freebayes:
	input:
		bam = ALN_DIR + "/PAUL4_WGS.bam",
		index = ALN_DIR + "/PAUL4_WGS.bam.bai",
		ref = REF_DIR + "/LHISI_Scaffold_Assembly_Polished.fasta",
		regions = REF_DIR + "/bed_files/genome.{chrom}.region.{i}.bed"
	output:
		OUT_DIR + "/{chrom}/variants.{i}.vcf"
	params:
		soft = SOFT_DIR
	threads: 1
	shell:
		"""
		{params.soft}/freebayes_1.3.6 \
		-f {input.ref} \
		-t {input.regions} \
		--use-best-n-alleles 2 \
    --report-monomorphic \
    -0 --min-coverage 10 -g 30 \
		{input.bam} > {output}
		"""

# Rule to filter vcfs, treating variant and invariant sites separately
rule filter_vcfs:
	input: OUT_DIR + "/{chrom}/variants.{i}.vcf"
	output:
		out_het = OUT_DIR + "/{chrom}/variants.{i}.het.vcf",
		out_hom = OUT_DIR + "/{chrom}/variants.{i}.hom.vcf",
	threads:2
	params:
		dir = OUT_DIR,
		ref_dir = REF_DIR
	shell:
		"""
		cd {params.dir}

		# Intersect specific bedfile with repeats bed file to reduce file-reading for vcftools

		bedtools intersect -a {params.ref_dir}/bed_files/genome.{wildcards.chrom}.region.{wildcards.i}.bed -b {params.ref_dir}/Annotation/repeats.bed > filter_{wildcards.chrom}_{wildcards.i}.bed

		# Filter by depth first
		# This will significantly reduce the burden of later sections
		# Permissive filter

		vcftools --vcf {input} \
		--min-meanDP 15 --max-meanDP 25 \
		--remove-indels \
		--exclude-bed filter_{wildcards.chrom}_{wildcards.i}.bed \
		--recode --recode-INFO-all --out depthfilt_{wildcards.chrom}_{wildcards.i}

		rm filter_{wildcards.chrom}_{wildcards.i}.bed

		# Split into header and tailer

		grep "#" depthfilt_{wildcards.chrom}_{wildcards.i}.recode.vcf > {wildcards.chrom}_{wildcards.i}_header
		grep -v "#" depthfilt_{wildcards.chrom}_{wildcards.i}.recode.vcf > {wildcards.chrom}_{wildcards.i}_tailer
		rm depthfilt_{wildcards.chrom}_{wildcards.i}.recode.vcf

		# Split tailer into hom and het

		grep "0/0\|1/1\|0|0\|1|1" {wildcards.chrom}_{wildcards.i}_tailer > {wildcards.chrom}_{wildcards.i}_hom
		grep "0/1\|1/0\|0|1\|1|0" {wildcards.chrom}_{wildcards.i}_tailer > {wildcards.chrom}_{wildcards.i}_het

		# Make full hom vcf

		cat {wildcards.chrom}_{wildcards.i}_header {wildcards.chrom}_{wildcards.i}_hom > {output.out_hom}

		# Get full het vcf for filtering

		cat {wildcards.chrom}_{wildcards.i}_header {wildcards.chrom}_{wildcards.i}_het > {wildcards.chrom}_{wildcards.i}_het.vcf

		# Remove tmp files

		rm {wildcards.chrom}_{wildcards.i}_tailer {wildcards.chrom}_{wildcards.i}_het {wildcards.chrom}_{wildcards.i}_hom

		# Filter variant VCF, easy part

		vcffilter -f 'AB > 0.2 & AB < 0.8 & QUAL / DP > 0.25' {wildcards.chrom}_{wildcards.i}_het.vcf | grep -v "#" > {output.out_het}

		# Remove other tmp files

		rm {wildcards.chrom}_{wildcards.i}_het.vcf
		"""

# Rule to concat calls into chromosome specific files
rule concat_vcfs:
	input:
		calls_het = expand(OUT_DIR + "/{{chrom}}/variants.{i}.het.vcf", i=CHUNKS),
		calls_hom = expand(OUT_DIR + "/{{chrom}}/variants.{i}.hom.vcf", i=CHUNKS),
		index = REF_DIR + "/LHISI_Scaffold_Assembly_Polished.fasta.fai"
	output:
		out_het = OUT_DIR + "/PAUL4_Het_{chrom}.vcf",
		out_hom = OUT_DIR + "/PAUL4_Hom_{chrom}.vcf"
	threads:4
	params:
		dir = OUT_DIR
	shell:
		"""
		cd {params.dir}

		# We have to separate processing the header into two lines to avoid 141 error
		# This occurs when a pipe is still streaming after the following command exits

		cat {input.calls_hom} | awk '/#/' > tmp2_{wildcards.chrom}
		awk '1;/CHROM/{{exit}}' tmp2_{wildcards.chrom} >> head_{wildcards.chrom}

		cat {input.calls_het} | awk '!/#/' | sort -k2,2n > het_{wildcards.chrom}
		cat head_{wildcards.chrom} het_{wildcards.chrom} | vcfuniq > {output.out_het}

		cat {input.calls_hom} | awk '!/#/' | sort -k2,2n > hom_{wildcards.chrom}
		cat head_{wildcards.chrom} hom_{wildcards.chrom} | vcfuniq > {output.out_hom}

		rm tmp2_{wildcards.chrom} head_{wildcards.chrom} het_{wildcards.chrom} hom_{wildcards.chrom}

		rm -rf {wildcards.chrom}
		"""

## Rule to get table of per-scaffold heterozygosity in sliding windows
rule H:
	input:
		het = OUT_DIR + "/PAUL4_Het_{chrom}.vcf",
		hom = OUT_DIR + "/PAUL4_Hom_{chrom}.vcf"
	output: OUT_DIR + "/H_count_{chrom}.txt"
	params:
		ref_dir = REF_DIR,
		dir = OUT_DIR
	shell:
		"""
		cd {params.dir}

		# Get length of scaffold
		cut -f1,2 {params.ref_dir}/LHISI_Scaffold_Assembly_Polished.fasta.fai | grep {wildcards.chrom} > {wildcards.chrom}_length

		# Make bedfile
		bedtools makewindows -g {wildcards.chrom}_length -w 1000000 -s 500000 > {wildcards.chrom}_windows.bed

		# Get file of site positions for variant, invariant
		awk '!/#/' {input.het} | cut -f1,2 | awk '{{print $1"\t"$2-1"\t"$2}}'	> {wildcards.chrom}_het.bed
		awk '!/#/' {input.hom} | cut -f1,2 | awk '{}{print $1"\t"$2-1"\t"$2}}'	> {wildcards.chrom}_hom.bed

		# Now we intersect each to get the count of variant and invariant sites per window
		bedtools intersect \
		-a {wildcards.chrom}_windows.bed \
		-b {wildcards.chrom}_het.bed \
		-c | \
		cut -f4 > {wildcards.chrom}_het_count

		bedtools intersect \
		-a {wildcards.chrom}_windows.bed \
		-b {wildcards.chrom}_hom.bed \
		-c > {wildcards.chrom}_hom_count

		paste {wildcards.chrom}_hom_count {wildcards.chrom}_het_count > {output}

		rm {wildcards.chrom}_*bed {wildcards.chrom}_*count {wildcards.chrom}_length
		"""

rule combine_H:
	input: expand(OUT_DIR + "/H_count_{chrom}.txt",chrom=CHROMS)
	output: OUT_DIR + "/GENOME_WIDE_H.txt"
	params:
		dir = OUT_DIR
	shell:
		"""
		cd {params.dir}

		cat {input} > {output}

		rm {input}
		"""

rule plink_ROH_format:
	input:
		het = OUT_DIR + "/PAUL4_Het_{chrom}.vcf",
		hom = OUT_DIR + "/PAUL4_Hom_{chrom}.vcf"
	output: OUT_DIR + "/PAUL4_{chrom}.bed"
	params:
		dir = OUT_DIR
	shell:
		"""
		cd {params.dir}

		#Get header
		cat {input.het} | awk '/#/' > tmp2_{wildcards.chrom}
		awk '1;/CHROM/{{exit}}' tmp2_{wildcards.chrom} >> head_{wildcards.chrom}

		# Get tail
		cat {input.het} {input.hom} | awk '!/#/' | sort -k2,2n > tail_{wildcards.chrom}

		# Cat into single vcf
		cat head_{wildcards.chrom} tail_{wildcards.chrom} > full_{wildcards.chrom}.vcf

		# Clean up tmp outputs
		rm tmp2_{wildcards.chrom} tail_{wildcards.chrom} head_{wildcards.chrom}

		# Make plink format
		plink --make-bed --keep-allele-order --double-id --allow-extra-chr --vcf full_{wildcards.chrom}.vcf --out PAUL4_{wildcards.chrom}

		# Clean up tmp outputs
		rm full_{wildcards.chrom}.vcf
		"""

# run per chromosome
# Taken from JRobinson github
# ${PLINK} --bfile ${VCF%.vcf.gz} --out ${VCF%.vcf.gz}_ROH --homozyg

# rule plink_ROH_combine:
# combine per chromosome files into single file

# rule psmc_format:
# take variant
# combine into single for whole genome
# turn into psmc format

# rule psmc:
# run psmc on whole genome
# stop here to check the number of recombination events and readjust

# rule psmc_bootstrap:
# bootstrap psmc
# possibly doesn't need to be parallelised
