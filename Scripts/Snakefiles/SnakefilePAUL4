### Snakefile for PAUL4 analysis. The pipeline takes reads, maps and
### deduplicates them, calls variants in parallel with FreeBayes, and filters
### them. Then it converts the filtered vcfs into psmc and plink formats. It
### takes the plink format and identifies ROH across a few different parameter
### settings. It takes the psmc format and performs psmc for the whole genome
### and then performs 100 bootstraps. Finally, it calculates H for each
### scaffold.

### May add a rule to estimate recombination map using ismc.

HOME_DIR = "/mnt/data/dayhoff/home/scratch/groups/mikheyev/LHISI"
READ_DIR = HOME_DIR + "/Reads/WGS"
ALN_DIR = HOME_DIR + "/Alignments/WGS"
REF_DIR = HOME_DIR + "/References"
OUT_DIR = HOME_DIR + "/Analysis/PAUL4"
SOFT_DIR = "/mnt/data/dayhoff/home/u6905905/"

### Modify this line to exclude or include scaffolds as necessary
### Only include autosomes for this
#CHROMS = ["CM056993.1","CM056994.1","CM056995.1","CM056996.1","CM056997.1","CM056998.1","CM056999.1","CM057000.1","CM057001.1","CM057002.1","CM057003.1","CM057004.1","CM057005.1","CM057006.1","CM057007.1","CM057008.1"]
CHROMS = ["CM057008.1"] # tester line
### Define chunks for calling later
NCHUNKS = 20
CHUNKS = list(range(1,NCHUNKS+1))

### Final rule
rule all:
	input: expand(OUT_DIR + "/FILTERED_PAUL4_{chrom}.vcf",chrom=CHROMS)

### Map the files to the reference and follow samtools pipeline to remove duplicates
rule map:
	input:
		ref = REF_DIR + "/LHISI_Scaffold_Assembly_Polished.fasta",
		r1 = READ_DIR + "/PAUL4_R1.fastq.gz",
		r2 = READ_DIR + "/PAUL4_R2.fastq.gz"
	output:
		ALN_DIR + "/PAUL4_WGS.bam"
	params:
		dir = ALN_DIR
	threads: 46
	shell:
		"""
		bwa mem -t {threads} -R "@RG\\tID:PAUL4\\tSM:PAUL4\\tLB:NEXTFLEX\\tPL:ILLUMINA" \
		{input.ref} {input.r1} {input.r2} |
		samtools fixmate -m -r -u - - | \
		samtools sort -u -@ {threads} -T {params.dir}/PAUL4_WGS_1 - | \
		samtools markdup -@ {threads} -T {params.dir}/PAUL4_WGS_2 --reference {input.ref} -r -f {params.dir}/PAUL4_WGS_dupstats.txt - - | \
		samtools view -bh > {output}
		"""

### Index the bam files
rule index:
	input:
		ALN_DIR + "/PAUL4_WGS.bam"
	output:
		ALN_DIR + "/PAUL4_WGS.bam.bai"
	threads: 46
	shell:
		"""
		samtools index -@ {threads} {input}
		"""

### Rule to get depth statistics
rule get_depth:
	input:
		bam = ALN_DIR + "/PAUL4_WGS.bam",
		index = ALN_DIR + "/PAUL4_WGS.bam.bai"
	output:
		OUT_DIR + "/PAUL4_WGS.regions.bed.gz"
	threads: 46
	shell:
		"""
		mosdepth -t {threads} -n -Q 10 -b 10000 PAUL4_WGS {input.bam}
		rm *mosdepth*
		"""

### Rule to make bed files to split up calling
rule generate_freebayes_regions:
	input:
		ref_idx = REF_DIR + "/LHISI_Scaffold_Assembly_Polished.fasta",
		index = REF_DIR + "/LHISI_Scaffold_Assembly_Polished.fasta.fai",
	output:
		regions = expand(REF_DIR + "/bed_files/genome.{chrom}.region.{i}.bed", chrom=CHROMS, i=CHUNKS)
	params:
		chroms = CHROMS,
		chunks = NCHUNKS,
		soft = SOFT_DIR,
		ref_dir = REF_DIR
	shell:
		"""
		{params.soft}/freebayes/scripts/fasta_generate_regions.py \
		--chunks \
		--chromosomes {params.chroms} \
		--bed {params.ref_dir}/bed_files/genome \
		{input.index} \
		{params.chunks}
		"""

### Rule to do calls
rule variant_calling_freebayes:
	input:
		bam = ALN_DIR + "/PAUL4_WGS.bam",
		index = ALN_DIR + "/PAUL4_WGS.bam.bai",
		ref = REF_DIR + "/LHISI_Scaffold_Assembly_Polished.fasta",
		regions = REF_DIR + "/bed_files/genome.{chrom}.region.{i}.bed"
	output:
		OUT_DIR + "/{chrom}/variants.{i}.vcf"
	params:
		soft = SOFT_DIR
	threads: 1
	shell:
		"""
		{params.soft}/freebayes_1.3.6 \
		-f {input.ref} \
		-t {input.regions} \
		--use-best-n-alleles 2 \
    --report-monomorphic \
    -0 --min-coverage 10 -g 30 \
		{input.bam} > {output}
		"""

### Rule to concat calls into chromosome specific directories
rule concat_vcfs:
	input:
		calls = expand(OUT_DIR + "/{{chrom}}/variants.{i}.vcf", i=CHUNKS),
		index = REF_DIR + "/LHISI_Scaffold_Assembly_Polished.fasta.fai"
	output: OUT_DIR + "/PAUL4_Raw_{chrom}.vcf"
	threads:4
	params:
		dir = OUT_DIR
	shell:
		"""
		cd {params.dir}

		# We have to separate processing the header into two lines to avoid 141 error
		# This occurs when a pipe is still streaming after the following command exits
		cat {input.calls} | awk '/#/' > tmp2_{wildcards.chrom}
		awk '1;/CHROM/{{exit}}' tmp2_{wildcards.chrom} >> head_{wildcards.chrom}

		cat {input.calls} | awk '!/#/' | sort -k2,2n > tail_{wildcards.chrom}

		cat head_{wildcards.chrom} tail_{wildcards.chrom} | vcfuniq > {output}

		rm -rf tmp*_{wildcards.chrom} head_{wildcards.chrom} tail_{wildcards.chrom} {wildcards.chrom}
		"""

# Rule to filter vcfs, treating variant and invariant sites separately
rule filter_vcfs:
	input: OUT_DIR + "/PAUL4_Raw_{chrom}.vcf"
	output: OUT_DIR + "/FILTERED_PAUL4_{chrom}.vcf"
	threads:2
	params:
		dir = OUT_DIR
	shell:
		"""
		cd {params.dir}

		# Filter by depth first
		# This will significantly reduce the burden of later sections
		# Permissive filter

		### ADD IN STEP TO FILTER REPEATS

		vcftools --vcf {input} \
		--min-meanDP 15 --max-meanDP 25 \
		--remove-indels \
		--recode --recode-INFO-all --out depthfilt_{wildcards.chrom}

		# Split into header, variant, and invariant

		grep "#" depthfilt_{wildcards.chrom}.recode.vcf > {wildcards.chrom}_header
		grep -v "#" depthfilt_{wildcards.chrom}.recode.vcf > {wildcards.chrom}_tailer
		rm depthfilt_{wildcards.chrom}.recode.vcf

		grep "0/0\|1/1" {wildcards.chrom}_tailer > {wildcards.chrom}_tmp1

		grep "0/1\|1/0" {wildcards.chrom}_tailer > {wildcards.chrom}_tmp2
		cat {wildcards.chrom}_header {wildcards.chrom}_tmp2 > variant_depthfilt_{wildcards.chrom}.vcf
		rm {wildcards.chrom}_tmp2

		rm {wildcards.chrom}_tailer

		# Filter variant VCF, easy part

		vcffilter -f 'AB > 0.2 & AB < 0.8 & QUAL / DP > 0.25' variant_depthfilt_{wildcards.chrom}.vcf | grep -v "#" > {wildcards.chrom}_tmp2

		# These may have been true homozygotes
		# The imbalanced filtering of het/hom sites due to information imbalance worries me
		# So I'm recording how much information is lost when hets are filtered
		# Perhaps I can scale down the number of homozygous sites by a proportional amount to account for this
		# Or perhaps I can calculate metrics assuming these removed sites were true homozygotes

		NUM_REMAINING=$(cat {wildcards.chrom}_tmp2 | wc -l)
		NUM_TOTAL=$(grep -v "#" variant_depthfilt_{wildcards.chrom}.vcf | wc -l)
		echo "{wildcards.chrom}\t${{NUM_REMAINING}}\t${{NUM_TOTAL}}" >> {wildcards.chrom}_het_filtering.log

		# Now cat together, sort

		cat {wildcards.chrom}_tmp1 {wildcards.chrom}_tmp2 | sort -k2,2n > {wildcards.chrom}_tmp3
		rm {wildcards.chrom}_tmp1 {wildcards.chrom}_tmp2

		cat {wildcards.chrom}_header {wildcards.chrom}_tmp3 > FILTERED_PAUL4_{wildcards.chrom}.vcf
		rm {wildcards.chrom}_tmp3 {wildcards.chrom}_header variant_depthfilt_{wildcards.chrom}.vcf
		"""

# mask repeats
# split out into variant and invariant
# filter variant by normal metrics
# filter invariant by depth and quality etc.

# rule plink_format:
# combine variant and invariant per chromosome
# turn into plink format

# rule plink_ROH:
# run per chromosome

# rule plink_ROH_combine:
# combine per chromosome files into single file

# rule psmc_format:
# take variant
# combine into single for whole genome
# turn into psmc format

# rule psmc:
# run psmc on whole genome
# stop here to check the number of recombination events and readjust

# rule psmc_bootstrap:
# bootstrap psmc
# possibly doesn't need to be parallelised

# rule H:
# combine variant and invariant
# generate regions file
# generate count table for regions of variant and invariant

# rule combine H:
# combine per chromosome tables into single table for plotting
